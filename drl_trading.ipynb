{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oaDoHbxVH0CW"
      },
      "source": [
        "# Deep Q-Learning Applied to Algorithmic Trading\n",
        "\n",
        "<a href=\"https://www.kaggle.com/addarm/unsupervised-learning-as-signals-for-pairs-trading\" target=\"_blank\"><img align=\"left\" alt=\"Kaggle\" title=\"Open in Kaggle\" src=\"https://kaggle.com/static/images/open-in-kaggle.svg\"></a>\n",
        "\n",
        "\n",
        "<a href=\"https://colab.research.google.com/github/adamd1985/Deep-Q-Learning-Applied-to-Algorithmic-Trading/blob/main/drl_trading.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aM59cTClH0CZ"
      },
      "source": [
        "INTRO\n",
        "\n",
        "\n",
        "This deep learning network was inspired by the paper:\n",
        "```BibTeX\n",
        "@article{theate2021application,\n",
        "  title={An application of deep reinforcement learning to algorithmic trading},\n",
        "  author={Th{\\'e}ate, Thibaut and Ernst, Damien},\n",
        "  journal={Expert Systems with Applications},\n",
        "  volume={173},\n",
        "  pages={114632},\n",
        "  year={2021},\n",
        "  publisher={Elsevier}\n",
        "}\n",
        "```"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q4-GoceIIfT_",
        "outputId": "2e550a1c-510a-42eb-b15c-fdb883a9538a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\r0% [Working]\r            \rGet:1 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r0% [Connecting to archive.ubuntu.com (185.125.190.39)] [Connecting to security.\r                                                                               \rGet:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease [1,581 B]\n",
            "Get:3 http://security.ubuntu.com/ubuntu jammy-security InRelease [110 kB]\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates InRelease [119 kB]\n",
            "Get:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  Packages [736 kB]\n",
            "Hit:7 https://ppa.launchpadcontent.net/c2d4u.team/c2d4u4.0+/ubuntu jammy InRelease\n",
            "Hit:8 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:10 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Get:12 http://security.ubuntu.com/ubuntu jammy-security/main amd64 Packages [1,564 kB]\n",
            "Get:13 http://security.ubuntu.com/ubuntu jammy-security/restricted amd64 Packages [1,954 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 Packages [1,846 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy-updates/restricted amd64 Packages [1,997 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 Packages [1,349 kB]\n",
            "Fetched 9,680 kB in 3s (2,860 kB/s)\n",
            "Reading package lists... Done\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  freeglut3 libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev libgles-dev\n",
            "  libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev libglvnd-dev\n",
            "  libglx-dev libice-dev libopengl-dev libsm-dev libxfont2 libxkbfile1\n",
            "  libxt-dev x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "Suggested packages:\n",
            "  libice-doc libsm-doc libxt-doc\n",
            "The following NEW packages will be installed:\n",
            "  freeglut3 freeglut3-dev libegl-dev libfontenc1 libgl-dev libgl1-mesa-dev\n",
            "  libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev libglvnd-core-dev\n",
            "  libglvnd-dev libglx-dev libice-dev libopengl-dev libsm-dev libxfont2\n",
            "  libxkbfile1 libxt-dev x11-xkb-utils xfonts-base xfonts-encodings\n",
            "  xfonts-utils xserver-common xvfb\n",
            "0 upgraded, 25 newly installed, 0 to remove and 38 not upgraded.\n",
            "Need to get 9,075 kB of archives.\n",
            "After this operation, 18.7 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3 amd64 2.8.1-6 [74.0 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.2 [6,842 B]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libice-dev amd64 2:1.0.10-1build2 [51.4 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libsm-dev amd64 2:1.2.3-1build2 [18.1 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/universe amd64 freeglut3-dev amd64 2.8.1-6 [126 kB]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.8 [28.6 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.8 [863 kB]\n",
            "Fetched 9,075 kB in 4s (2,471 kB/s)\n",
            "debconf: unable to initialize frontend: Dialog\n",
            "debconf: (No usable dialog-like program is installed, so the dialog based frontend cannot be used. at /usr/share/perl5/Debconf/FrontEnd/Dialog.pm line 78, <> line 25.)\n",
            "debconf: falling back to frontend: Readline\n",
            "debconf: unable to initialize frontend: Readline\n",
            "debconf: (This frontend requires a controlling tty.)\n",
            "debconf: falling back to frontend: Teletype\n",
            "dpkg-preconfigure: unable to re-open stdin: \n",
            "Selecting previously unselected package freeglut3:amd64.\n",
            "(Reading database ... 121752 files and directories currently installed.)\n",
            "Preparing to unpack .../00-freeglut3_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../01-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../02-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../03-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../04-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../05-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../06-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../07-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../08-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../09-libgl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.2_amd64.deb ...\n",
            "Unpacking libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../10-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../11-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libice-dev:amd64.\n",
            "Preparing to unpack .../12-libice-dev_2%3a1.0.10-1build2_amd64.deb ...\n",
            "Unpacking libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Selecting previously unselected package libsm-dev:amd64.\n",
            "Preparing to unpack .../13-libsm-dev_2%3a1.2.3-1build2_amd64.deb ...\n",
            "Unpacking libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../14-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package freeglut3-dev:amd64.\n",
            "Preparing to unpack .../15-freeglut3-dev_2.8.1-6_amd64.deb ...\n",
            "Unpacking freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "Preparing to unpack .../16-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../17-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../18-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../19-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../20-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../21-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../22-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../23-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.8_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../24-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.8_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Setting up freeglut3:amd64 (2.8.1-6) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libice-dev:amd64 (2:1.0.10-1build2) ...\n",
            "Setting up libsm-dev:amd64 (2:1.2.3-1build2) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.8) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libgl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.2) ...\n",
            "Setting up freeglut3-dev:amd64 (2.8.1-6) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.4) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n",
            "Collecting tf-agents[reverb]\n",
            "  Downloading tf_agents-0.19.0-py3-none-any.whl (1.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m6.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: absl-py>=0.6.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.4.0)\n",
            "Requirement already satisfied: cloudpickle>=1.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (2.2.1)\n",
            "Requirement already satisfied: gin-config>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.5.0)\n",
            "Collecting gym<=0.23.0,>=0.17.0 (from tf-agents[reverb])\n",
            "  Downloading gym-0.23.0.tar.gz (624 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m624.4/624.4 kB\u001b[0m \u001b[31m18.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.23.5)\n",
            "Requirement already satisfied: pillow in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (9.4.0)\n",
            "Requirement already satisfied: six>=1.10.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.16.0)\n",
            "Requirement already satisfied: protobuf>=3.11.3 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (3.20.3)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (1.14.1)\n",
            "Collecting typing-extensions==4.5.0 (from tf-agents[reverb])\n",
            "  Downloading typing_extensions-4.5.0-py3-none-any.whl (27 kB)\n",
            "Collecting pygame==2.1.3 (from tf-agents[reverb])\n",
            "  Downloading pygame-2.1.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m39.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tensorflow-probability~=0.23.0 in /usr/local/lib/python3.10/dist-packages (from tf-agents[reverb]) (0.23.0)\n",
            "Collecting rlds (from tf-agents[reverb])\n",
            "  Downloading rlds-0.1.8-py3-none-manylinux2010_x86_64.whl (48 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m48.4/48.4 kB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting dm-reverb~=0.14.0 (from tf-agents[reverb])\n",
            "  Downloading dm_reverb-0.14.0-cp310-cp310-manylinux2014_x86_64.whl (6.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.4/6.4 MB\u001b[0m \u001b[31m42.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow~=2.15.0 (from tf-agents[reverb])\n",
            "  Downloading tensorflow-2.15.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (475.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m475.2/475.2 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: dm-tree in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (0.1.8)\n",
            "Requirement already satisfied: portpicker in /usr/local/lib/python3.10/dist-packages (from dm-reverb~=0.14.0->tf-agents[reverb]) (1.5.2)\n",
            "Requirement already satisfied: gym-notices>=0.0.4 in /usr/local/lib/python3.10/dist-packages (from gym<=0.23.0,>=0.17.0->tf-agents[reverb]) (0.0.8)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (16.0.6)\n",
            "Collecting ml-dtypes~=0.3.1 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading ml_dtypes-0.3.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m51.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (23.2)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (67.7.2)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (2.4.0)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow~=2.15.0->tf-agents[reverb]) (1.62.0)\n",
            "Collecting tensorboard<2.16,>=2.15 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading tensorboard-2.15.2-py3-none-any.whl (5.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.5/5.5 MB\u001b[0m \u001b[31m43.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tensorflow-estimator<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading tensorflow_estimator-2.15.0-py2.py3-none-any.whl (441 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m442.0/442.0 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting keras<2.16,>=2.15.0 (from tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading keras-2.15.0-py3-none-any.whl (1.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m37.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: decorator in /usr/local/lib/python3.10/dist-packages (from tensorflow-probability~=0.23.0->tf-agents[reverb]) (4.4.2)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow~=2.15.0->tf-agents[reverb]) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.27.0)\n",
            "Collecting google-auth-oauthlib<2,>=0.5 (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb])\n",
            "  Downloading google_auth_oauthlib-1.2.0-py2.py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.0.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from portpicker->dm-reverb~=0.14.0->tf-agents[reverb]) (5.9.5)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow~=2.15.0->tf-agents[reverb]) (3.2.2)\n",
            "Building wheels for collected packages: gym\n",
            "  Building wheel for gym (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for gym: filename=gym-0.23.0-py3-none-any.whl size=697631 sha256=d8d2fb4de8b8dffea5d5a3324a88a21ccc3fe63c9705791ada6efee13f7e951b\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/6f/b4/3991d4fae11d0ecb0754c11cc1b4e7745012850da4efaaf0b1\n",
            "Successfully built gym\n",
            "Installing collected packages: typing-extensions, tensorflow-estimator, rlds, pygame, ml-dtypes, keras, gym, tf-agents, dm-reverb, google-auth-oauthlib, tensorboard, tensorflow\n",
            "  Attempting uninstall: typing-extensions\n",
            "    Found existing installation: typing_extensions 4.10.0\n",
            "    Uninstalling typing_extensions-4.10.0:\n",
            "      Successfully uninstalled typing_extensions-4.10.0\n",
            "  Attempting uninstall: tensorflow-estimator\n",
            "    Found existing installation: tensorflow-estimator 2.12.0\n",
            "    Uninstalling tensorflow-estimator-2.12.0:\n",
            "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
            "  Attempting uninstall: pygame\n",
            "    Found existing installation: pygame 2.5.2\n",
            "    Uninstalling pygame-2.5.2:\n",
            "      Successfully uninstalled pygame-2.5.2\n",
            "  Attempting uninstall: ml-dtypes\n",
            "    Found existing installation: ml-dtypes 0.2.0\n",
            "    Uninstalling ml-dtypes-0.2.0:\n",
            "      Successfully uninstalled ml-dtypes-0.2.0\n",
            "  Attempting uninstall: keras\n",
            "    Found existing installation: keras 2.12.0\n",
            "    Uninstalling keras-2.12.0:\n",
            "      Successfully uninstalled keras-2.12.0\n",
            "  Attempting uninstall: gym\n",
            "    Found existing installation: gym 0.25.2\n",
            "    Uninstalling gym-0.25.2:\n",
            "      Successfully uninstalled gym-0.25.2\n",
            "  Attempting uninstall: google-auth-oauthlib\n",
            "    Found existing installation: google-auth-oauthlib 0.4.6\n",
            "    Uninstalling google-auth-oauthlib-0.4.6:\n",
            "      Successfully uninstalled google-auth-oauthlib-0.4.6\n",
            "  Attempting uninstall: tensorboard\n",
            "    Found existing installation: tensorboard 2.12.0\n",
            "    Uninstalling tensorboard-2.12.0:\n",
            "      Successfully uninstalled tensorboard-2.12.0\n",
            "  Attempting uninstall: tensorflow\n",
            "    Found existing installation: tensorflow 2.12.0\n",
            "    Uninstalling tensorflow-2.12.0:\n",
            "      Successfully uninstalled tensorflow-2.12.0\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "sqlalchemy 2.0.28 requires typing-extensions>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "orbax-checkpoint 0.4.4 requires jax>=0.4.9, but you have jax 0.3.25 which is incompatible.\n",
            "pydantic 2.6.3 requires typing-extensions>=4.6.1, but you have typing-extensions 4.5.0 which is incompatible.\n",
            "pydantic-core 2.16.3 requires typing-extensions!=4.7.0,>=4.6.0, but you have typing-extensions 4.5.0 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed dm-reverb-0.14.0 google-auth-oauthlib-1.2.0 gym-0.23.0 keras-2.15.0 ml-dtypes-0.3.2 pygame-2.1.3 rlds-0.1.8 tensorboard-2.15.2 tensorflow-2.15.1 tensorflow-estimator-2.15.0 tf-agents-0.19.0 typing-extensions-4.5.0\n",
            "Collecting pyglet\n",
            "  Downloading pyglet-2.0.14-py3-none-any.whl (876 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m876.8/876.8 kB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pyglet\n",
            "Successfully installed pyglet-2.0.14\n",
            "Requirement already satisfied: tf-keras in /usr/local/lib/python3.10/dist-packages (2.15.0)\n",
            "\u001b[31mERROR: Could not find a version that satisfies the requirement shutil (from versions: none)\u001b[0m\u001b[31m\n",
            "\u001b[0m\u001b[31mERROR: No matching distribution found for shutil\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "try:\n",
        "  import google.colab\n",
        "\n",
        "  !sudo apt-get update\n",
        "  !sudo apt-get install -y xvfb freeglut3-dev\n",
        "  !pip install pyvirtualdisplay\n",
        "  !pip install tf-agents[reverb]\n",
        "  !pip install pyglet\n",
        "  !pip install tf-keras\n",
        "  !pip install shutil\n",
        "\n",
        "  from google.colab import files\n",
        "  IN_COLAB = True\n",
        "except:\n",
        "  IN_COLAB = False\n",
        "  files = None"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C39UfWDmzYvL",
        "outputId": "d8072f02-53f8-493b-f5d3-7c0a96392e2c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Running in Kaggle or Collab...\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.10/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.23.5)\n",
            "Requirement already satisfied: scipy>=1.3.2 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (1.3.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn) (3.3.0)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.10/dist-packages (2.15.1)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=23.5.26 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (24.3.6)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.4.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: h5py>=2.9.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.9.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (16.0.6)\n",
            "Requirement already satisfied: ml-dtypes~=0.3.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.3.2)\n",
            "Requirement already satisfied: numpy<2.0.0,>=1.23.5 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.23.5)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.3.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from tensorflow) (23.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.20.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (3.20.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from tensorflow) (67.7.2)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.16.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.4.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (4.5.0)\n",
            "Requirement already satisfied: wrapt<1.15,>=1.11.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.14.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (0.36.0)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (1.62.0)\n",
            "Requirement already satisfied: tensorboard<2.16,>=2.15 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.2)\n",
            "Requirement already satisfied: tensorflow-estimator<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: keras<2.16,>=2.15.0 in /usr/local/lib/python3.10/dist-packages (from tensorflow) (2.15.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.10/dist-packages (from astunparse>=1.6.0->tensorflow) (0.42.0)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.5.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (2.31.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard<2.16,>=2.15->tensorflow) (3.0.1)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.3.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard<2.16,>=2.15->tensorflow) (2024.2.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard<2.16,>=2.15->tensorflow) (2.1.5)\n",
            "Requirement already satisfied: pyasn1<0.6.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.16,>=2.15->tensorflow) (0.5.1)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard<2.16,>=2.15->tensorflow) (3.2.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (4.66.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (3.7.1)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (4.49.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.4.5)\n",
            "Requirement already satisfied: numpy>=1.20 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (1.23.5)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (23.2)\n",
            "Requirement already satisfied: pillow>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (9.4.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib) (2.8.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.7->matplotlib) (1.16.0)\n",
            "Collecting python-dotenv\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Installing collected packages: python-dotenv\n",
            "Successfully installed python-dotenv-1.0.1\n",
            "Requirement already satisfied: yfinance in /usr/local/lib/python3.10/dist-packages (0.2.37)\n",
            "Requirement already satisfied: pandas>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.5.3)\n",
            "Requirement already satisfied: numpy>=1.16.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.23.5)\n",
            "Requirement already satisfied: requests>=2.31 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.31.0)\n",
            "Requirement already satisfied: multitasking>=0.0.7 in /usr/local/lib/python3.10/dist-packages (from yfinance) (0.0.11)\n",
            "Requirement already satisfied: lxml>=4.9.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.9.4)\n",
            "Requirement already satisfied: appdirs>=1.4.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.4.4)\n",
            "Requirement already satisfied: pytz>=2022.5 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2023.4)\n",
            "Requirement already satisfied: frozendict>=2.3.4 in /usr/local/lib/python3.10/dist-packages (from yfinance) (2.4.0)\n",
            "Requirement already satisfied: peewee>=3.16.2 in /usr/local/lib/python3.10/dist-packages (from yfinance) (3.17.1)\n",
            "Requirement already satisfied: beautifulsoup4>=4.11.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (4.12.3)\n",
            "Requirement already satisfied: html5lib>=1.1 in /usr/local/lib/python3.10/dist-packages (from yfinance) (1.1)\n",
            "Requirement already satisfied: soupsieve>1.2 in /usr/local/lib/python3.10/dist-packages (from beautifulsoup4>=4.11.1->yfinance) (2.5)\n",
            "Requirement already satisfied: six>=1.9 in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (1.16.0)\n",
            "Requirement already satisfied: webencodings in /usr/local/lib/python3.10/dist-packages (from html5lib>=1.1->yfinance) (0.5.1)\n",
            "Requirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas>=1.3.0->yfinance) (2.8.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (3.6)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.31->yfinance) (2024.2.2)\n",
            "Requirement already satisfied: pyarrow in /usr/local/lib/python3.10/dist-packages (14.0.2)\n",
            "Requirement already satisfied: numpy>=1.16.6 in /usr/local/lib/python3.10/dist-packages (from pyarrow) (1.23.5)\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "os.environ['TF_USE_LEGACY_KERAS'] = '1' # KERAS 2 only for tfagents\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "import shutil\n",
        "\n",
        "IS_KAGGLE = os.getenv('IS_KAGGLE', 'True') == 'True'\n",
        "if IN_COLAB or IS_KAGGLE:\n",
        "    # Kaggle confgs\n",
        "    print('Running in Kaggle or Collab...')\n",
        "    %pip install scikit-learn\n",
        "    %pip install tensorflow\n",
        "    %pip install tqdm\n",
        "    %pip install matplotlib\n",
        "    %pip install python-dotenv\n",
        "    %pip install yfinance\n",
        "    %pip install pyarrow\n",
        "    for dirname, _, filenames in os.walk('/kaggle/input'):\n",
        "        for filename in filenames:\n",
        "            print(os.path.join(dirname, filename))\n",
        "\n",
        "    DATA_DIR = \"/kaggle/input/DATASET\"\n",
        "else:\n",
        "    DATA_DIR = \"./data/\"\n",
        "    print('Running Local...')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Mh5TWk3l1CJ-"
      },
      "outputs": [],
      "source": [
        "START_DATE = \"2017-01-01\"\n",
        "SPLIT_DATE = '2018-1-1' # Turning point from train to tst\n",
        "END_DATE = \"2019-12-31\" # pd.Timestamp(datetime.now() - BDay(1)).strftime('%Y-%m-%d')\n",
        "DATA_DIR = \"./data\"\n",
        "INDEX = \"Date\"\n",
        "TARGET = 'TSLA'\n",
        "TICKER_SYMBOLS = [TARGET]\n",
        "INTERVAL = \"1d\"\n",
        "\n",
        "MODELS_PATH = './models'\n",
        "LOGS_PATH = './logs'\n",
        "\n",
        "ACT_NEUTRAL = 3 # Added this action to go to neutral and wait\n",
        "ACT_LONG = 2\n",
        "ACT_HOLD = 1\n",
        "ACT_SHORT = 0\n",
        "\n",
        "CAPITAL = 100000\n",
        "TRADE_COSTS_PERCENT = 0.1 / 100\n",
        "\n",
        "FEATURES = [\"Close\", \"High\", \"Low\", \"Open\", \"Volume\"]\n",
        "TARGET_FEATURE = \"Price Raw\"\n",
        "STATE_LEN = 1\n",
        "OBS_SPACE = (STATE_LEN)*len(FEATURES)\n",
        "ACT_SPACE = 2\n",
        "\n",
        "BATCH_SIZE = OBS_SPACE * 1000\n",
        "LEARN_RATE = 1e-3\n",
        "TOTAL_ITERS = 10000\n",
        "EPISODES = 10\n",
        "INIT_COLLECT = 100\n",
        "TOTAL_COLLECT = 1\n",
        "LOG_INTERVALS = 200\n",
        "TEST_INTERVALS = 1000\n",
        "MEMORY_LENGTH = OBS_SPACE * 100\n",
        "DISCOUNT = 0.4\n",
        "EPSILON_START = 1.\n",
        "EPSILON_END = 0.01\n",
        "EPSILON_DECAY = 10000\n",
        "ALPHA = 0.1\n",
        "L2Factor = 0.000001\n",
        "NEURONS = 512\n",
        "LAYERS = (NEURONS, NEURONS, NEURONS, NEURONS, NEURONS)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H4DpIfIPH0Ch"
      },
      "source": [
        "# Financial Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "GJiIs_h-H0Ca",
        "outputId": "3426f707-7355-49e9-8e9a-d3eecccc9083"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'/content'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 11
        }
      ],
      "source": [
        "import numpy as np\n",
        "import math\n",
        "import yfinance as yf\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from datetime import datetime\n",
        "from pandas.tseries.offsets import BDay\n",
        "\n",
        "from scipy.stats import skew, kurtosis\n",
        "\n",
        "import tensorflow as tf\n",
        "from tf_agents.specs import array_spec, tensor_spec\n",
        "from tf_agents.trajectories import time_step as ts\n",
        "from tf_agents.utils import common\n",
        "from tf_agents.metrics import tf_metrics\n",
        "from tf_agents.eval import metric_utils\n",
        "from tf_agents.agents.dqn import dqn_agent\n",
        "from tf_agents.drivers import py_driver\n",
        "from tf_agents.environments import suite_gym, py_environment, tf_py_environment, utils\n",
        "from tf_agents.networks import sequential\n",
        "from tf_agents.policies import py_tf_eager_policy, policy_saver, random_tf_policy\n",
        "\n",
        "import reverb\n",
        "from tf_agents.replay_buffers import reverb_replay_buffer, reverb_utils\n",
        "\n",
        "os.getcwd()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 473
        },
        "id": "s64pmt9mH0Cj",
        "outputId": "dc54ecd9-ac58-4f9b-8bf7-207654fb39de"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TSLA => min_date: 2017-01-03 00:00:00, max_date: 2019-12-30 00:00:00, kurt:-0.56, skewness:-0.28, outliers_count:0,  nan_count: 0\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 Open       High        Low      Close  Adj Close     Volume\n",
              "Date                                                                        \n",
              "2017-01-03  14.324000  14.688667  14.064000  14.466000  14.466000   88849500\n",
              "2017-01-04  14.316667  15.200000  14.287333  15.132667  15.132667  168202500\n",
              "2017-01-05  15.094667  15.165333  14.796667  15.116667  15.116667   88675500\n",
              "2017-01-06  15.128667  15.354000  15.030000  15.267333  15.267333   82918500\n",
              "2017-01-09  15.264667  15.461333  15.200000  15.418667  15.418667   59692500\n",
              "...               ...        ...        ...        ...        ...        ...\n",
              "2019-12-23  27.452000  28.134001  27.333332  27.948000  27.948000  199794000\n",
              "2019-12-24  27.890667  28.364668  27.512667  28.350000  28.350000  120820500\n",
              "2019-12-26  28.527332  28.898666  28.423332  28.729334  28.729334  159508500\n",
              "2019-12-27  29.000000  29.020666  28.407333  28.691999  28.691999  149185500\n",
              "2019-12-30  28.586000  28.600000  27.284000  27.646667  27.646667  188796000\n",
              "\n",
              "[753 rows x 6 columns]"
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-f277c66b-1e06-46d6-b17b-4e1491698816\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Open</th>\n",
              "      <th>High</th>\n",
              "      <th>Low</th>\n",
              "      <th>Close</th>\n",
              "      <th>Adj Close</th>\n",
              "      <th>Volume</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Date</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2017-01-03</th>\n",
              "      <td>14.324000</td>\n",
              "      <td>14.688667</td>\n",
              "      <td>14.064000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>14.466000</td>\n",
              "      <td>88849500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-04</th>\n",
              "      <td>14.316667</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>14.287333</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>15.132667</td>\n",
              "      <td>168202500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-05</th>\n",
              "      <td>15.094667</td>\n",
              "      <td>15.165333</td>\n",
              "      <td>14.796667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>15.116667</td>\n",
              "      <td>88675500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-06</th>\n",
              "      <td>15.128667</td>\n",
              "      <td>15.354000</td>\n",
              "      <td>15.030000</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>15.267333</td>\n",
              "      <td>82918500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2017-01-09</th>\n",
              "      <td>15.264667</td>\n",
              "      <td>15.461333</td>\n",
              "      <td>15.200000</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>15.418667</td>\n",
              "      <td>59692500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-23</th>\n",
              "      <td>27.452000</td>\n",
              "      <td>28.134001</td>\n",
              "      <td>27.333332</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>27.948000</td>\n",
              "      <td>199794000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-24</th>\n",
              "      <td>27.890667</td>\n",
              "      <td>28.364668</td>\n",
              "      <td>27.512667</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>28.350000</td>\n",
              "      <td>120820500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-26</th>\n",
              "      <td>28.527332</td>\n",
              "      <td>28.898666</td>\n",
              "      <td>28.423332</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>28.729334</td>\n",
              "      <td>159508500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-27</th>\n",
              "      <td>29.000000</td>\n",
              "      <td>29.020666</td>\n",
              "      <td>28.407333</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>28.691999</td>\n",
              "      <td>149185500</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2019-12-30</th>\n",
              "      <td>28.586000</td>\n",
              "      <td>28.600000</td>\n",
              "      <td>27.284000</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>27.646667</td>\n",
              "      <td>188796000</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>753 rows × 6 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-f277c66b-1e06-46d6-b17b-4e1491698816')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-f277c66b-1e06-46d6-b17b-4e1491698816 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-f277c66b-1e06-46d6-b17b-4e1491698816');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "<div id=\"df-c559c73f-6848-446d-b2b5-90f95378c2e9\">\n",
              "  <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-c559c73f-6848-446d-b2b5-90f95378c2e9')\"\n",
              "            title=\"Suggest charts\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "  </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "  <script>\n",
              "    async function quickchart(key) {\n",
              "      const quickchartButtonEl =\n",
              "        document.querySelector('#' + key + ' button');\n",
              "      quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "      quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "      try {\n",
              "        const charts = await google.colab.kernel.invokeFunction(\n",
              "            'suggestCharts', [key], {});\n",
              "      } catch (error) {\n",
              "        console.error('Error during call to suggestCharts:', error);\n",
              "      }\n",
              "      quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "      quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "    }\n",
              "    (() => {\n",
              "      let quickchartButtonEl =\n",
              "        document.querySelector('#df-c559c73f-6848-446d-b2b5-90f95378c2e9 button');\n",
              "      quickchartButtonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "    })();\n",
              "  </script>\n",
              "</div>\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "dataframe",
              "summary": "{\n  \"name\": \"tickers[TARGET]\",\n  \"rows\": 753,\n  \"fields\": [\n    {\n      \"column\": \"Date\",\n      \"properties\": {\n        \"dtype\": \"date\",\n        \"min\": \"2017-01-03 00:00:00\",\n        \"max\": \"2019-12-30 00:00:00\",\n        \"num_unique_values\": 753,\n        \"samples\": [\n          \"2019-11-06 00:00:00\",\n          \"2019-08-06 00:00:00\",\n          \"2018-06-25 00:00:00\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Open\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0998823778838096,\n        \"min\": 12.073332786560059,\n        \"max\": 29.0,\n        \"num_unique_values\": 711,\n        \"samples\": [\n          20.118667602539062,\n          21.391332626342773,\n          20.06800079345703\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"High\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.1384437712983404,\n        \"min\": 12.445332527160645,\n        \"max\": 29.020666122436523,\n        \"num_unique_values\": 708,\n        \"samples\": [\n          25.796667098999023,\n          21.05466651916504,\n          19.631332397460938\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Low\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.052082343134819,\n        \"min\": 11.799332618713379,\n        \"max\": 28.42333221435547,\n        \"num_unique_values\": 721,\n        \"samples\": [\n          23.399999618530273,\n          15.915332794189453,\n          20.600000381469727\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0939495816459504,\n        \"min\": 11.9313325881958,\n        \"max\": 28.729333877563477,\n        \"num_unique_values\": 735,\n        \"samples\": [\n          17.89466667175293,\n          18.492666244506836,\n          21.487333297729492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Adj Close\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 3.0939495816459504,\n        \"min\": 11.9313325881958,\n        \"max\": 28.729333877563477,\n        \"num_unique_values\": 735,\n        \"samples\": [\n          17.89466667175293,\n          18.492666244506836,\n          21.487333297729492\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"Volume\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 66969490,\n        \"min\": 32800500,\n        \"max\": 504745500,\n        \"num_unique_values\": 751,\n        \"samples\": [\n          108093000,\n          89928000,\n          84378000\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ],
      "source": [
        "def get_tickerdata(tickers_symbols, start=START_DATE, end=END_DATE, interval=INTERVAL, datadir=DATA_DIR):\n",
        "    tickers = {}\n",
        "    earliest_end= datetime.strptime(end,'%Y-%m-%d')\n",
        "    latest_start = datetime.strptime(start,'%Y-%m-%d')\n",
        "    os.makedirs(DATA_DIR, exist_ok=True)\n",
        "    for symbol in tickers_symbols:\n",
        "        cached_file_path = f\"{datadir}/{symbol}-{start}-{end}-{interval}.csv\"\n",
        "\n",
        "        try:\n",
        "            if os.path.exists(cached_file_path):\n",
        "                df = pd.read_parquet(cached_file_path)\n",
        "                df.index = pd.to_datetime(df.index)\n",
        "                assert len(df) > 0\n",
        "            else:\n",
        "                df = yf.download(\n",
        "                    symbol,\n",
        "                    start=START_DATE,\n",
        "                    end=END_DATE,\n",
        "                    progress=False,\n",
        "                    interval=INTERVAL,\n",
        "                )\n",
        "                assert len(df) > 0\n",
        "                df.to_parquet(cached_file_path, index=True, compression=\"snappy\")\n",
        "            min_date = df.index.min()\n",
        "            max_date = df.index.max()\n",
        "            nan_count = df[\"Close\"].isnull().sum()\n",
        "            skewness = round(skew(df[\"Close\"].dropna()), 2)\n",
        "            kurt = round(kurtosis(df[\"Close\"].dropna()), 2)\n",
        "            outliers_count = (df[\"Close\"] > df[\"Close\"].mean() + (3 * df[\"Close\"].std())).sum()\n",
        "            print(\n",
        "                f\"{symbol} => min_date: {min_date}, max_date: {max_date}, kurt:{kurt}, skewness:{skewness}, outliers_count:{outliers_count},  nan_count: {nan_count}\"\n",
        "            )\n",
        "            tickers[symbol] = df\n",
        "\n",
        "            if min_date > latest_start:\n",
        "                latest_start = min_date\n",
        "            if max_date < earliest_end:\n",
        "                earliest_end = max_date\n",
        "        except Exception as e:\n",
        "            print(f\"Error with {symbol}: {e}\")\n",
        "\n",
        "    return tickers, latest_start, earliest_end\n",
        "\n",
        "tickers, latest_start, earliest_end = get_tickerdata(TICKER_SYMBOLS)\n",
        "tickers[TARGET]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lke4koO5H0Cl"
      },
      "source": [
        "# Trading Environment"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dXLhk-CKH0Co",
        "outputId": "be881cfc-5b56-42e3-8651-e5b6ecbb48d7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "TimeStep Specs: TimeStep(\n",
            "{'step_type': ArraySpec(shape=(), dtype=dtype('int32'), name='step_type'),\n",
            " 'reward': ArraySpec(shape=(), dtype=dtype('float32'), name='reward'),\n",
            " 'discount': BoundedArraySpec(shape=(), dtype=dtype('float32'), name='discount', minimum=0.0, maximum=1.0),\n",
            " 'observation': BoundedArraySpec(shape=(5,), dtype=dtype('float32'), name='observation', minimum=-3.4028234663852886e+38, maximum=3.4028234663852886e+38)})\n",
            "Action Specs: BoundedArraySpec(shape=(), dtype=dtype('int32'), name='action', minimum=0, maximum=2)\n",
            "Reward Specs: ArraySpec(shape=(), dtype=dtype('float32'), name='reward')\n",
            "Time step: TimeStep(\n",
            "{'step_type': array(0, dtype=int32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 1.9550354 ,  1.6273892 ,  0.6523866 , -0.08771443,  1.6908462 ],\n",
            "      dtype=float32)})\n",
            "Next time step: TimeStep(\n",
            "{'step_type': array(1, dtype=int32),\n",
            " 'reward': array(0., dtype=float32),\n",
            " 'discount': array(1., dtype=float32),\n",
            " 'observation': array([ 1.9550354 ,  1.6273892 ,  0.6523866 , -0.08771443,  1.6908462 ],\n",
            "      dtype=float32)})\n"
          ]
        }
      ],
      "source": [
        "class TradingEnv(py_environment.PyEnvironment):\n",
        "    \"\"\"\n",
        "    A custom trading environment for reinforcement learning, compatible with tf_agents.\n",
        "\n",
        "    This environment simulates a simple trading scenario where an agent can take one of three actions:\n",
        "    - Long (buy), Short (sell), or Hold a financial instrument, aiming to maximize profit through trading decisions.\n",
        "\n",
        "    Parameters:\n",
        "    - data: DataFrame containing the stock market data.\n",
        "    - data_dim: Dimension of the data to be used for each observation.\n",
        "    - money: Initial capital to start trading.\n",
        "    - state_length: Number of past observations to consider for the state.\n",
        "    - transaction_cost: Costs associated with trading actions.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, data, features = FEATURES, money=CAPITAL, state_length=STATE_LEN, transaction_cost=0, market_costs=TRADE_COSTS_PERCENT):\n",
        "        super(TradingEnv, self).__init__()\n",
        "\n",
        "        assert data is not None\n",
        "\n",
        "        self.features = features\n",
        "        self.data_dim = len(self.features)\n",
        "        self.current_step = 0\n",
        "\n",
        "        self.balance = money\n",
        "        self.initial_balance = money\n",
        "        self.transaction_cost = transaction_cost\n",
        "        self.epsilon = max(market_costs, np.finfo(float).eps) # there is always volatility costs\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self.state_length = state_length\n",
        "        self._episode_ended = False\n",
        "        self._batch_size = 1\n",
        "        self._action_spec = array_spec.BoundedArraySpec(shape=(), dtype=np.int32, minimum=ACT_SHORT, maximum=ACT_LONG, name='action')\n",
        "        self._observation_spec = array_spec.BoundedArraySpec(shape=(self.data_dim,), dtype=np.float32, name='observation')\n",
        "\n",
        "        self.data = self.preprocess_data(data.copy())\n",
        "\n",
        "        self.reset()\n",
        "\n",
        "    @property\n",
        "    def batched(self):\n",
        "        return False #True\n",
        "\n",
        "    @property\n",
        "    def batch_size(self):\n",
        "        return None #self._batch_size\n",
        "\n",
        "    @batch_size.setter\n",
        "    def batch_size(self, size):\n",
        "        self._batch_size = size\n",
        "\n",
        "    def preprocess_data(self, df):\n",
        "        def _log_rets(df):\n",
        "            log_returns = np.log(df / df.shift(1))\n",
        "            df = (log_returns - log_returns.mean()) / log_returns.std()\n",
        "            df = df.dropna()\n",
        "            return df\n",
        "\n",
        "        price_raw = df['Close'].copy()\n",
        "        df[self.features] = _log_rets(df[self.features])\n",
        "        l = df[self.features]\n",
        "        df = df.replace(0.0, np.nan)\n",
        "        df = df.interpolate(method='linear', limit=5, limit_area='inside')\n",
        "        df = df.ffill().bfill()\n",
        "\n",
        "        df[TARGET_FEATURE] = price_raw\n",
        "        df['Position'] = 0\n",
        "        df['Action'] = ACT_HOLD\n",
        "        df['Holdings'] = 0.\n",
        "        df['Cash'] = float(self.balance)\n",
        "        df['Money'] = df['Holdings'] + df['Cash']\n",
        "        df['Returns'] = 0.\n",
        "\n",
        "        assert not df.isna().any().any()\n",
        "\n",
        "        return df\n",
        "\n",
        "    def action_spec(self):\n",
        "        \"\"\"Provides the specification of the action space.\"\"\"\n",
        "        return self._action_spec\n",
        "\n",
        "    def observation_spec(self):\n",
        "        \"\"\"Provides the specification of the observation space.\"\"\"\n",
        "        return self._observation_spec\n",
        "\n",
        "    def _reset(self):\n",
        "        \"\"\"Resets the environment state and prepares for a new episode.\"\"\"\n",
        "        self.balance = self.initial_balance\n",
        "        self.current_step = 0\n",
        "        self._episode_ended = False\n",
        "        self.total_shares = 0\n",
        "\n",
        "        self.data['Position'] = 0\n",
        "        self.data['Action'] = ACT_HOLD\n",
        "        self.data['Holdings'] = 0.\n",
        "        self.data['Cash']  = float(self.balance)\n",
        "        self.data['Money'] = self.data.iloc[0]['Holdings'] + self.data.iloc[0]['Cash']\n",
        "        self.data['Returns'] = 0.\n",
        "\n",
        "        initial_observation = self._next_observation()\n",
        "        return ts.restart(initial_observation)\n",
        "\n",
        "    def _next_observation(self):\n",
        "        \"\"\"Generates the next observation based on the current step.\"\"\"\n",
        "        if self.current_step == 0 or self.state_length == 1:\n",
        "            obs = self.data[self.features].iloc[0: self.current_step + 1]\n",
        "        else:\n",
        "            obs = self.data[self.features].iloc[min(0, self.current_step-self.state_length):self.current_step]\n",
        "            assert len(obs) == self.state_length\n",
        "        obs = obs.values[0]\n",
        "        obs = obs.flatten().astype(np.float32)\n",
        "        return obs\n",
        "\n",
        "    def _step(self, action):\n",
        "        \"\"\"Executes a trading action and updates the environment's state.\"\"\"\n",
        "        if self._episode_ended:\n",
        "            return self.reset()\n",
        "\n",
        "        self.current_step += 1\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        current_price = self.data.iloc[self.current_step][TARGET_FEATURE]\n",
        "\n",
        "        if self.current_step == 250 or  self.data.iloc[self.current_step].isna().any().any():\n",
        "            assert not self.data.iloc[self.current_step].isna().any().any()\n",
        "\n",
        "        if action == ACT_LONG:\n",
        "            self._process_long_position(current_price)\n",
        "        elif action == ACT_SHORT:\n",
        "            prev_current_price = self.data.iloc[self.current_step - 1][TARGET_FEATURE]\n",
        "            self._process_short_position(current_price, prev_current_price)\n",
        "        elif action == ACT_HOLD:\n",
        "            self._process_hold_position()\n",
        "        elif action == ACT_NEUTRAL:\n",
        "            self._process_neutral_position(current_price)\n",
        "        else:\n",
        "          raise Exception(f\"Invalid Actions: {action}\")\n",
        "\n",
        "        self._update_financials(current_price)\n",
        "        done = self.current_step >= len(self.data) - 1\n",
        "        reward = self._calculate_reward()\n",
        "        if done:\n",
        "            self._episode_ended = True\n",
        "            return ts.termination(self._next_observation(), reward)\n",
        "        else:\n",
        "            return ts.transition(self._next_observation(), reward)\n",
        "\n",
        "    def _get_lower_bound(self, cash, total_shares, price):\n",
        "        \"\"\"\n",
        "        Compute the lower bound of the action space, particularly for short selling,\n",
        "        based on current cash, the number of shares, and the current price.\n",
        "        \"\"\"\n",
        "        delta = -cash - total_shares * price * (1 + self.epsilon) * (1 + self.transaction_cost)\n",
        "\n",
        "        if delta < 0:\n",
        "            lowerBound = delta / (price * (2 * self.transaction_cost + self.epsilon * (1 + self.transaction_cost)))\n",
        "        else:\n",
        "            lowerBound = delta / (price * self.epsilon * (1 + self.transaction_cost))\n",
        "\n",
        "        if np.isinf(lowerBound):\n",
        "            assert False\n",
        "        return lowerBound\n",
        "\n",
        "    def _process_hold_position(self):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "        self.data.at[step_idx, \"Holdings\"] = self.data.iloc[self.current_step - 1][\"Holdings\"]\n",
        "        self.data.at[step_idx, \"Position\"] = self.data.iloc[self.current_step - 1][\"Position\"]\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_HOLD\n",
        "\n",
        "    def _process_neutral_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, \"Cash\"] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "        self.data.at[step_idx, \"Holdings\"] = 0.0\n",
        "        self.data.at[step_idx, \"Position\"] = 0.0\n",
        "        self.data.at[step_idx, \"Action\"] = ACT_NEUTRAL\n",
        "\n",
        "    def _process_long_position(self, current_price):\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = 1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == 1:\n",
        "            # more long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash']\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new long\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "        else:\n",
        "            # short to long\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step]['Cash'] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step]['Cash'] - self.total_shares * current_price * (1 + self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = 1\n",
        "\n",
        "    def _process_short_position(self, current_price, prev_price):\n",
        "        \"\"\"\n",
        "        Adjusts the logic for processing short positions to include lower bound calculations.\n",
        "        \"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "        self.data.at[step_idx, 'Position'] = -1\n",
        "        if self.data.iloc[self.current_step - 1]['Position'] == -1:\n",
        "            # Short more\n",
        "            low = self._get_lower_bound(self.data.iloc[self.current_step - 1]['Cash'], -self.total_shares, prev_price)\n",
        "            if low <= 0:\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"]\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            else:\n",
        "                total_sharesToBuy = min(math.floor(low), self.total_shares)\n",
        "                self.total_shares -= total_sharesToBuy\n",
        "                self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] - total_sharesToBuy * current_price * (1 + self.transaction_cost)\n",
        "                self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "        elif self.data.iloc[self.current_step - 1]['Position'] == 0:\n",
        "            # new short\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step - 1][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "        else:\n",
        "            # long to short\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step - 1][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.total_shares = math.floor(self.data.iloc[self.current_step][\"Cash\"] / (current_price * (1 + self.transaction_cost)))\n",
        "            self.data.at[step_idx, 'Cash'] = self.data.iloc[self.current_step][\"Cash\"] + self.total_shares * current_price * (1 - self.transaction_cost)\n",
        "            self.data.at[step_idx, 'Holdings'] = -self.total_shares * current_price\n",
        "            self.data.at[step_idx, 'Action'] = -1\n",
        "\n",
        "    def _update_financials(self, current_price):\n",
        "        \"\"\"Updates the financial metrics including cash, money, and returns.\"\"\"\n",
        "        step_idx = self.data.index[self.current_step]\n",
        "\n",
        "        self.data.at[step_idx,'Money'] = self.data.iloc[self.current_step]['Holdings'] + self.data.iloc[self.current_step]['Cash']\n",
        "        self.data.at[step_idx,'Returns'] = ((self.data.iloc[self.current_step]['Money'] - self.data.iloc[self.current_step - 1]['Money'])) / self.data.iloc[self.current_step - 1]['Money']\n",
        "\n",
        "    def _calculate_reward(self, reward_clip=1000):\n",
        "        \"\"\"\n",
        "        Calculates the reward for the current step.\n",
        "        \"\"\"\n",
        "        return np.clip(self.data.iloc[self.current_step]['Returns'], -reward_clip, reward_clip)\n",
        "\n",
        "    def render(self, mode='human'):\n",
        "        print(f'Step: {self.current_step}, Balance: {self.balance}')\n",
        "\n",
        "stock= tickers[TARGET]\n",
        "train_data = stock[stock.index < pd.to_datetime(SPLIT_DATE)].copy()\n",
        "test_data = stock[stock.index >= pd.to_datetime(SPLIT_DATE)].copy()\n",
        "\n",
        "train_env = TradingEnv(train_data)\n",
        "utils.validate_py_environment(train_env, episodes=EPISODES)\n",
        "test_env = TradingEnv(test_data)\n",
        "utils.validate_py_environment(train_env, episodes=EPISODES//4)\n",
        "\n",
        "print(f\"TimeStep Specs: {train_env.time_step_spec()}\")\n",
        "print(f\"Action Specs: {train_env.action_spec()}\")\n",
        "print(f\"Reward Specs: {train_env.time_step_spec().reward}\")\n",
        "\n",
        "time_step = train_env.reset()\n",
        "print(f'Time step: {time_step}')\n",
        "action = np.array(ACT_HOLD, dtype=np.int32)\n",
        "next_time_step = train_env.step(action)\n",
        "print(f'Next time step: {next_time_step}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "viOTgMvkPLkO"
      },
      "source": [
        "# The Problem Definition\n",
        "\n",
        "We are teaching an agent to trade in an environment with many unknowns. Our objective to make sequential interaction that lead to the highest sharpe ratio.\n",
        "\n",
        "Let's formulize our policy, which is finding the optimal action *a_t* given state *s_t* to maximize our expected reward *r_t*:\n",
        "\n",
        "$$\n",
        "\\pi^*(a_t | s_t) = \\arg\\max_{a \\in \\mathcal{A}} \\mathbb{E} \\left[ \\sum_{t=0}^{\\infty} \\gamma^t r_t \\middle| s_t = f(o_1, a_1, r_1, \\ldots, o_t), a_t \\right]\n",
        "$$\n",
        "\n",
        "At each timestep *t*:\n",
        "\n",
        "1. Observe the environments state *s_t* and map history with *f(.)*\n",
        "2. Observations *o_t* from history *h_t*, have previous actions *a_t-1*, previous observations *o_t-1* and their returns *r_t-1*. For our experiment, we'll encode these into features for a network.\n",
        "3. Execute action *a_t*, which can be: hold, long, short\n",
        "4. Get returns *r_t* discounted at *γ_t*. *γ* is the discounting factor to prevent the agent from doing only tactical choices for returns in the present (missing better future returns).\n",
        "\n",
        "\n",
        "The *π(a_t|h_t)* creates an action on quantity *at = Qt*. Where a positive *Q* is the long, the negative *Q* signals a short and when its 0 no action is taken.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V8Roh_T5bs1c"
      },
      "source": [
        "## Actions and Rewards\n",
        "\n",
        "A core concept in RL is rewards engineering.\n",
        "\n",
        "Let's look at our action space *A* at time *t*:\n",
        "\n",
        "$$\n",
        "a_t = Q_t \\in \\{Q_{\\text{Long}, t}, Q_{\\text{Short}, t}\\}\n",
        "$$\n",
        "\n",
        "The action *Q_Long,t* is set to maximize returns on a buy, given our liquidity *vc_t* (the value *v* of our portfolio with cash remainng *c*) and purchasing *Q_long* at price *p* shares (transaction costs *C*) if we are not already long:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Long}, t} =\n",
        "\\begin{cases}\n",
        "\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Long}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "The action *Q_Short,t* aims to convert a **negative** number of shares to returns (shorting is the borrowing of shares, therefore our *v_c* will be initially negative).\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} =\n",
        "\\begin{cases}\n",
        "-2n_t - \\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)}\\right\\rfloor & \\text{if } a_{t-1} \\neq Q_{\\text{Short}, t-1}, \\\\\n",
        "0 & \\text{otherwise}.\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "Note the *-2n* is an indication to sell twice, meaning not close a long position but open a short position, also for the *Qn* shares, we need to negate the amount we can buy, as its a short position. If we had no shares to start, then *-2(0)* will not have an effect save for the short amount:\n",
        "\n",
        "$$\n",
        "\\hat{Q}_{\\text{Short}, t} = -\\left\\lfloor \\frac{v_{c,t}}{p_t (1 + C)} \\right\\rfloor\n",
        "$$\n",
        "\n",
        "We need to have boundaries, as a short can have infinite loss:\n",
        "\n",
        "$$\n",
        "Q_{\\text{Short}, t} = \\max\\{\\hat{Q}_{\\text{Short}, t}, Q_t\\}\n",
        "$$\n",
        "\n",
        "Given that our portfolio cannot fall into negative amounts, we need to model constraints.\n",
        "1. Cash value *vc_t* needs to be large enough to return to neutral *n_t=0*.\n",
        "2. To return to 0, we need to adjust for costs *C* of market volatility epsiloc *ϵ*.\n",
        "3. We redifine the action space permissable to ensure we can always return to neutral.\n",
        "\n",
        "$$\n",
        "v_{c,t+1} \\geq -n_{t+1} p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "The action space *A* is redefined as a set of acceptable values for *Q_t* between boundaries *Q-* and *Q+*:\n",
        "\n",
        "$$\n",
        "A = \\left\\{ Q_t \\in \\mathbb{Z} \\cap \\left[Q_t^-, Q_t^+\\right] \\right\\}\n",
        "$$\n",
        "\n",
        "Where the top boundary *Q+* is:\n",
        "$$\n",
        "Q_t^+ = \\frac{v_{c,t}}{p_t (1+C)}\n",
        "$$\n",
        "\n",
        "And the lower boundary *Q-* is (for both coming out of a long where delta *t* is positive, or reversing a short and incurring twice the costs with delta *t* in the negative):\n",
        "\n",
        "$$\n",
        "Q_t^- = \\begin{cases}\n",
        "    \\frac{\\Delta t}{p_t \\varepsilon (1 + C)} & \\text{if } \\Delta t \\geq 0, \\\\\n",
        "    \\frac{\\Delta t}{p_t (2C + \\varepsilon(1 + C))} & \\text{if } \\Delta t < 0,\n",
        "    \\end{cases}\n",
        "$$\n",
        "\n",
        "with *delta t* being the in change of portfolio value in time:\n",
        "\n",
        "$$\n",
        " t_Δ = -v_{c,t} - n_t p_t (1 + \\varepsilon)(1 + C)\n",
        "$$\n",
        "\n",
        "In the above boundaries, the cost of trading is defined as:\n",
        "\n",
        "$$\n",
        "v_{c,t+1} = v_{c,t} - Q_t p_t - C |Q_t| p_t\n",
        "$$\n",
        "\n",
        "Where *C* is the percentage cost of the transaction given the absolute quantity *|Q_t|* of shares and their price *p_t*."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GChhvmHGohQ2"
      },
      "source": [
        "## Agent's Objective\n",
        "\n",
        "As initially declared in this section, our agent's aim is to maximize the sharpe ratio:\n",
        "\n",
        "$$\n",
        "\\max_{\\pi} \\left( \\frac{E\\left[\\sum_{t=0}^{T} \\gamma^t r_t - R_f\\right]}{\\sqrt{\\mathrm{Var}\\left[\\sum_{t=0}^{T} \\gamma^t r_t\\right]}} \\right)\n",
        "$$\n",
        "\n",
        "which is just the maximization of:\n",
        "\n",
        "$$\n",
        "\\text{sharpe}= \\left( \\frac{\\bar{R} - R_f}{\\sigma} \\right)\n",
        "$$\n",
        "\n",
        "or the returns of the portfolio (annualized) minus the risk free rate (at the time of writing, 5%) divided by the volatility or standard deviation of the portfolio"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xcb7hq6yH0Cq"
      },
      "source": [
        "# Deep Q-Network Architecure"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_AIE_vPrH0Cr"
      },
      "source": [
        "## Architecture\n",
        "\n",
        "2 models:\n",
        "- Policy Model: This is the primary model that the agent uses to make decisions or select actions based on the current state of the environment. The policy model is actively trained and updated throughout the training process based on the agent's experiences. In real-life applications, after the training phase is complete, the policy model is what gets deployed to make decisions or take actions in the given environment.\n",
        "- Target Model: The target model is used exclusively during the training phase to provide a stable target for the temporal difference (TD) error calculation, which is crucial for the stability of the Q-learning updates. The target model's weights are periodically synchronized with the policy model's weights but at a much slower rate. This delayed update helps to stabilize the learning process by making the target for the policy updates more consistent across training batches. The target model itself is not used for decision-making or action selection outside of the training context.\n",
        "\n",
        "Some notes on this 2 model arch:\n",
        "- Stability/Reducing Temporal Correlations: The agent learns a policy that maps states to actions by using a Q-function. This Q-function estimates the rewards by taking a certain action in a given state. The learning process continuously updates the Q-values based on new experiences. If the Q-function is constantly changing—as it would be when updates are made based on estimates from the same function—it can lead to unstable training dynamics. The estimates can become overly optimistic, and the learning process can diverge.\n",
        "- Target: The target network is a stable baseline for the policy network to compare against. While the policy network is frequently updated to reflect the latest learning, the target network's weights are updated less frequently. This slower update rate provides a fixed target for the policy network to aim for over multiple iterations, making the learning process more stable.\n",
        "\n",
        "In practice, the policy network is responsible for selecting actions during training and gameplay. Its weights are regularly updated to reflect the agent's learning. The target network, on the other hand, is used to generate the Q-value targets for the updates of the policy network. Every few steps, the weights from the policy network are copied to the target network, ensuring the target for the updates remains relatively stable but still gradually adapts to the improved policy. The policy model is used both during training (for learning and decision-making) and after training (for decision-making in the deployment environment).\n",
        "\n",
        "The target model is used during the training process only, to calculate stable target values for updating the policy model.\n",
        "After training is complete and the model is deployed in a real-world application, only the policy model is used to make decisions or take actions based on the learned policy. The target model's role ends with the completion of the training phase, as its primary purpose is to aid in the convergence and stability of the training process itself."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q49V1zseH0Cs"
      },
      "source": [
        "## DRL Flow\n",
        "\n",
        "- Initialization: init policy network and the target network with the same architecture but separate parameters.\n",
        "- Data Preparation: Normalize input data using calculated coefficients to ensure consistency in scale.\n",
        "- Learning Process:\n",
        "    -At each training step, observe the current market state and process it through normalization.\n",
        "    - Select an action using the epsilon-greedy policy (a balance between exploration and exploitation) based on the current state.\n",
        "    - Execute the selected action in the simulated trading environment, observe the next state, and receive a reward based on the action's outcome.\n",
        "    - Store the experience (current state, action, reward, next state) in the replay memory.\n",
        "    - Sample a random batch of past experiences from the replay memory for learning to reduce correlation between consecutive learning steps.\n",
        "    - Use the policy network to predict Q-values for the current states and the target network to calculate the target Q-values for the next states.\n",
        "    - Update the policy network by minimizing the difference between its Q-value predictions and the target Q-values using backpropagation.\n",
        "    - Every few steps, update the target network's parameters with the policy network's parameters to gradually adapt the learning target.\n",
        "- Evaluation and Adjustment: Periodically test the trained policy network on a separate validation set or environment to evaluate performance.\n",
        "    -Repeat the learning and evaluation process for many episodes until the policy network stabilizes and performs satisfactorily.\n",
        "- Application Phase\n",
        "    - Model Deployment: Deploy the trained policy network in a real-world environment or a simulation that closely mimics real trading conditions.\n",
        "    - Real-time Operation: Observe the current market state and process it (normalization, etc.) as done during training.\n",
        "    - Use the trained policy network to select the action that maximizes expected rewards based on the current market state, leaning towards exploitation of the learned policy over exploration. Execute the selected action in the market (buy, sell, hold).\n",
        "- Continuous Learning:\n",
        "    - Repeat the learning process with new market data and experiences, possibly in a less frequent, offline manner.\n",
        "    - Update the policy and target networks as new data becomes available and as the market evolves to maintain or improve performance over time."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2y5vgYpxH0Cs",
        "outputId": "20e213cf-842c-44d5-e97b-b362a8894a1b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Num GPUs Available:  0\n",
            "<tf_agents.policies.greedy_policy.GreedyPolicy object at 0x7c3d687cd4e0>\n",
            "<tf_agents.policies.epsilon_greedy_policy.EpsilonGreedyPolicy object at 0x7c3d687cd300>\n"
          ]
        }
      ],
      "source": [
        "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
        "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
        "for gpu in tf.config.experimental.list_physical_devices('GPU'):\n",
        "    tf.config.experimental.set_memory_growth(gpu, True)\n",
        "\n",
        "def create_q_network(env, fc_layer_params = LAYERS):\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "    action_tensor_spec = tensor_spec.from_spec(env.action_spec())\n",
        "    num_actions = action_tensor_spec.maximum - action_tensor_spec.minimum + 1\n",
        "\n",
        "    def _dense_layer(num_units):\n",
        "        return tf.keras.layers.Dense(\n",
        "            num_units,\n",
        "            activation=tf.keras.activations.relu,\n",
        "            kernel_initializer=tf.keras.initializers.VarianceScaling(\n",
        "                scale=2.0, mode='fan_in', distribution='truncated_normal'))\n",
        "\n",
        "    dense_layers = [_dense_layer(num_units) for num_units in fc_layer_params]\n",
        "    q_values_layer = tf.keras.layers.Dense(\n",
        "        num_actions,\n",
        "        activation=None,\n",
        "        kernel_initializer=tf.keras.initializers.GlorotNormal(), # Xavier\n",
        "        bias_initializer=tf.keras.initializers.GlorotNormal())\n",
        "    q_net = sequential.Sequential(dense_layers + [q_values_layer])\n",
        "\n",
        "    return q_net\n",
        "\n",
        "def create_agent(q_net, env, t_q_net = None, train_step_counter=None, optimizer = tf.keras.optimizers.Adam(learning_rate=LEARN_RATE), eps=EPSILON_START, gradient_clipping = 1.):\n",
        "    env = tf_py_environment.TFPyEnvironment(env)\n",
        "\n",
        "    if train_step_counter is None:\n",
        "      train_step_counter = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "    # see: https://www.tensorflow.org/agents/api_docs/python/tf_agents/agents/DqnAgent\n",
        "    agent = dqn_agent.DqnAgent(\n",
        "        env.time_step_spec(),\n",
        "        env.action_spec(),\n",
        "        q_network=q_net,\n",
        "        target_q_network = t_q_net,\n",
        "        optimizer=optimizer,\n",
        "        epsilon_greedy = 0.5,\n",
        "        reward_scale_factor = 0.01,\n",
        "        gradient_clipping = gradient_clipping,\n",
        "        td_errors_loss_fn=common.element_wise_huber_loss,\n",
        "        train_step_counter=train_step_counter,\n",
        "        name=\"TradeAgent\")\n",
        "\n",
        "    agent.initialize()\n",
        "    print(agent.policy)\n",
        "    print(agent.collect_policy)\n",
        "    return agent, train_step_counter\n",
        "\n",
        "q_net = create_q_network(train_env)\n",
        "t_q_net = create_q_network(train_env)\n",
        "agent, train_step_counter = create_agent(q_net, train_env, t_q_net=t_q_net)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WXZ_bo04H0Ct"
      },
      "source": [
        "# Trading Operations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7DmbtWugH0Ct",
        "outputId": "4d3aff70-a586-46d8-f4e2-c2dc77e108a6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Trajectory(\n",
            "{'step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
            " 'observation': BoundedTensorSpec(shape=(5,), dtype=tf.float32, name='observation', minimum=array(-3.4028235e+38, dtype=float32), maximum=array(3.4028235e+38, dtype=float32)),\n",
            " 'action': BoundedTensorSpec(shape=(), dtype=tf.int32, name='action', minimum=array(0, dtype=int32), maximum=array(2, dtype=int32)),\n",
            " 'policy_info': (),\n",
            " 'next_step_type': TensorSpec(shape=(), dtype=tf.int32, name='step_type'),\n",
            " 'reward': TensorSpec(shape=(), dtype=tf.float32, name='reward'),\n",
            " 'discount': BoundedTensorSpec(shape=(), dtype=tf.float32, name='discount', minimum=array(0., dtype=float32), maximum=array(1., dtype=float32))})\n",
            "('step_type', 'observation', 'action', 'policy_info', 'next_step_type', 'reward', 'discount')\n",
            "Random Step 0: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 1: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 2: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([1], dtype=int32)>, state=(), info=())\n",
            "Random Step 3: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 4: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 5: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 6: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 7: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([2], dtype=int32)>, state=(), info=())\n",
            "Random Step 8: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n",
            "Random Step 9: PolicyStep(action=<tf.Tensor: shape=(1,), dtype=int32, numpy=array([0], dtype=int32)>, state=(), info=())\n"
          ]
        }
      ],
      "source": [
        "class TradingSimulator:\n",
        "    def __init__(self, env, eval_env, agent, episodes=EPISODES,\n",
        "                 batch_size=BATCH_SIZE, num_eval_episodes=TEST_INTERVALS,\n",
        "                 collect_steps_per_iteration=INIT_COLLECT,\n",
        "                 replay_buffer_max_length=MEMORY_LENGTH ,\n",
        "                 num_iterations = TOTAL_ITERS, log_interval=LOG_INTERVALS,\n",
        "                 eval_interval=TEST_INTERVALS, global_step=None):\n",
        "        self.py_env = env\n",
        "        self.env =  tf_py_environment.TFPyEnvironment(self.py_env)\n",
        "        self.py_eval_env = eval_env\n",
        "        self.eval_env =  tf_py_environment.TFPyEnvironment(self.py_eval_env)\n",
        "        self.agent = agent\n",
        "        self.episodes = episodes\n",
        "        self.log_interval = log_interval\n",
        "        self.eval_interval = eval_interval\n",
        "        self.global_step = global_step\n",
        "\n",
        "        self.batch_size = batch_size\n",
        "        self.num_eval_episodes = num_eval_episodes\n",
        "        self.collect_steps_per_iteration = collect_steps_per_iteration\n",
        "        self.replay_buffer_max_length = replay_buffer_max_length\n",
        "        self.num_iterations = num_iterations\n",
        "\n",
        "        self.policy = self.agent.policy\n",
        "        self.collect_policy = self.agent.collect_policy\n",
        "        self.random_policy = random_tf_policy.RandomTFPolicy(\n",
        "            self.env.time_step_spec(),\n",
        "            self.env.action_spec())\n",
        "\n",
        "        self.replay_buffer_signature = tensor_spec.from_spec(\n",
        "            self.agent.collect_data_spec)\n",
        "        self.replay_buffer_signature = tensor_spec.add_outer_dim(\n",
        "            self.replay_buffer_signature)\n",
        "\n",
        "    def init_memory(self, table_name = 'uniform_table'):\n",
        "        self.table = reverb.Table(\n",
        "            table_name,\n",
        "            max_size=self.replay_buffer_max_length,\n",
        "            sampler=reverb.selectors.Uniform(),\n",
        "            remover=reverb.selectors.Fifo(),\n",
        "            rate_limiter=reverb.rate_limiters.MinSize(1),\n",
        "            signature=self.replay_buffer_signature)\n",
        "\n",
        "        self.reverb_server = reverb.Server([self.table])\n",
        "        self.replay_buffer = reverb_replay_buffer.ReverbReplayBuffer(\n",
        "                                    self.agent.collect_data_spec,\n",
        "                                    table_name=table_name,\n",
        "                                    sequence_length=2,\n",
        "                                    local_server=self.reverb_server)\n",
        "\n",
        "        self.rb_observer = reverb_utils.ReverbAddTrajectoryObserver(self.replay_buffer.py_client, table_name, sequence_length=2)\n",
        "\n",
        "        print(self.agent.collect_data_spec)\n",
        "        print(self.agent.collect_data_spec._fields)\n",
        "\n",
        "        # Test with random actions\n",
        "        py_driver.PyDriver(\n",
        "            self.py_env,\n",
        "            py_tf_eager_policy.PyTFEagerPolicy(self.random_policy, True),\n",
        "            [self.rb_observer],\n",
        "            max_steps=self.collect_steps_per_iteration).run(self.py_env.reset())\n",
        "        time_step = self.env.reset()\n",
        "        for i in range(10):\n",
        "          action_step = self.random_policy.action(time_step)\n",
        "          time_step = self.env.step(action_step.action)\n",
        "          print(f\"Random Step {i}: {self.random_policy.action(time_step)}\")\n",
        "          if time_step.is_last():\n",
        "              break\n",
        "\n",
        "        self.compute_episode_metrics(self.eval_env, self.random_policy, self.num_eval_episodes)\n",
        "        self.dataset = self.replay_buffer.as_dataset(num_parallel_calls=3, sample_batch_size=self.batch_size, num_steps=2).prefetch(3)\n",
        "\n",
        "        iterator = iter(self.dataset)\n",
        "        print(iterator)\n",
        "\n",
        "        return self.dataset, iterator\n",
        "\n",
        "    def compute_episode_metrics(self, environment, policy, num_eval_episodes):\n",
        "        total_returns = []\n",
        "        episode_sharpe_ratios = []\n",
        "        episode_std_devs = []\n",
        "        episode_avg_returns = []\n",
        "\n",
        "        for _ in range(num_eval_episodes):\n",
        "            time_step = environment.reset()\n",
        "            episode_returns = []\n",
        "\n",
        "            while not time_step.is_last():\n",
        "                action_step = policy.action(time_step)\n",
        "                time_step = environment.step(action_step.action)\n",
        "                rewards = time_step.reward.numpy()\n",
        "                episode_returns.extend(rewards.flatten())\n",
        "\n",
        "            total_episode_return = np.sum(episode_returns)\n",
        "            total_returns.append(total_episode_return)\n",
        "            episode_avg_return = np.mean(episode_returns)\n",
        "            episode_avg_returns.append(episode_avg_return)\n",
        "            episode_std_dev = np.std(episode_returns)\n",
        "            episode_std_devs.append(episode_std_dev)\n",
        "            episode_sharpe_ratio = episode_avg_return / episode_std_dev if episode_std_dev > 0 else 0\n",
        "            episode_sharpe_ratios.append(episode_sharpe_ratio)\n",
        "\n",
        "        return np.max(total_returns), np.max(episode_avg_returns), np.min(episode_std_devs), np.max(episode_sharpe_ratios)\n",
        "\n",
        "    def train(self, checkpoint_path=MODELS_PATH, initial_epsilon= EPSILON_START, final_epsilon = EPSILON_END, decay_steps=EPSILON_DECAY):\n",
        "        _, iterator = self.init_memory()\n",
        "\n",
        "\n",
        "        if self.global_step is None:\n",
        "          self.global_step = tf.compat.v1.train.get_or_create_global_step()\n",
        "\n",
        "        checkpoint_dir = os.path.join(checkpoint_path, 'checkpoint')\n",
        "        train_checkpointer = common.Checkpointer(\n",
        "            ckpt_dir=checkpoint_dir,\n",
        "            max_to_keep=1,\n",
        "            agent=agent,\n",
        "            policy=agent.policy,\n",
        "            replay_buffer=self.replay_buffer,\n",
        "            global_step=self.global_step\n",
        "        )\n",
        "        train_checkpointer.initialize_or_restore()\n",
        "        self.global_step = tf.compat.v1.train.get_global_step()\n",
        "\n",
        "        self.agent.train = common.function(self.agent.train)\n",
        "        self.agent.train_step_counter.assign(self.global_step )\n",
        "\n",
        "        # Agent's first eval\n",
        "        total_returns, avg_return, std_dev, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.agent.policy, self.num_eval_episodes)\n",
        "        metrics = [[total_returns, avg_return, std_dev, sharpe_ratio]]\n",
        "\n",
        "        time_step = self.py_env.reset()\n",
        "        collect_driver = py_driver.PyDriver(\n",
        "            self.py_env,\n",
        "            py_tf_eager_policy.PyTFEagerPolicy(self.agent.collect_policy, use_tf_function=True),\n",
        "            [self.rb_observer],\n",
        "            max_steps=self.collect_steps_per_iteration)\n",
        "\n",
        "        for _ in tqdm(range(self.num_iterations), desc=\"Training Loop\"):\n",
        "            time_step, _ = collect_driver.run(time_step)\n",
        "            experience, _ = next(iterator)\n",
        "            train_loss = self.agent.train(experience).loss\n",
        "            step = self.agent.train_step_counter.numpy()\n",
        "\n",
        "            if step % self.log_interval == 0:\n",
        "                print('step = {0}: loss = {1}'.format(step, train_loss))\n",
        "                train_checkpointer.save(self.global_step)\n",
        "\n",
        "            if step % self.eval_interval == 0:\n",
        "                total_returns, avg_return, std_dev, sharpe_ratio = self.compute_episode_metrics(self.eval_env, self.agent.policy, self.num_eval_episodes)\n",
        "                print('step = {0}: Average Return = {1}, Total Return = {2}, Sharpe = {3}'.format(step, avg_return, total_returns, sharpe_ratio))\n",
        "                metrics.append([total_returns, avg_return, std_dev, sharpe_ratio])\n",
        "\n",
        "            # Later call: saved_policy = tf.saved_model.load(policy_dir)\n",
        "            train_checkpointer.save(self.global_step)\n",
        "\n",
        "            # This is epsilon decay\n",
        "            decayed_epsilon = final_epsilon + (initial_epsilon - final_epsilon) * \\\n",
        "                      np.exp(-1. * step / decay_steps)\n",
        "            agent.collect_policy._epsilon = epsilon\n",
        "\n",
        "        totals, average, _, sharpe_ratios = zip(*metrics)\n",
        "        print(f'\\nTraining completed. Mean Reward: {np.mean(average):.4f}, Mean Totals: {np.mean(totals):.4f}, Mean Loss: {np.mean(train_loss):.4f}, Mean Sharpe: {np.mean(sharpe_ratios):.4f}')\n",
        "\n",
        "        policy_dir = os.path.join(checkpoint_path, 'policy')\n",
        "        tf_policy_saver = policy_saver.PolicySaver(agent.policy)\n",
        "        tf_policy_saver.save(policy_dir)\n",
        "\n",
        "        return metrics\n",
        "\n",
        "    def plot_performance(self, metrics):\n",
        "        \"\"\"\n",
        "        Plot the training performance including average returns and Sharpe Ratios on the same plot,\n",
        "        with returns on the left y-axis and Sharpe Ratios on the right y-axis.\n",
        "        \"\"\"\n",
        "        _, average_returns, _, sharpe_ratios = zip(*metrics)  # Ignore std deviations as per your request\n",
        "        iterations = range(0, self.num_iterations + 1, self.eval_interval)\n",
        "        iterations = list(iterations)[:len(average_returns)]\n",
        "\n",
        "        fig, ax1 = plt.subplots(figsize=(10, 6))\n",
        "\n",
        "        color = 'tab:blue'\n",
        "        ax1.set_xlabel('Iterations')\n",
        "        ax1.set_ylabel('Average Return', color=color)\n",
        "        ax1.plot(iterations, average_returns, label='Average Return', color=color)\n",
        "        ax1.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        ax2 = ax1.twinx()\n",
        "        color = 'tab:green'\n",
        "        ax2.set_ylabel('Sharpe Ratio', color=color)\n",
        "        ax2.plot(iterations, sharpe_ratios, label='Sharpe Ratio', color=color)\n",
        "        ax2.tick_params(axis='y', labelcolor=color)\n",
        "\n",
        "        fig.tight_layout()\n",
        "        plt.title('Training Performance: Average Returns and Sharpe Ratios')\n",
        "        plt.show()\n",
        "\n",
        "sim = TradingSimulator(train_env, test_env, agent=agent, global_step=train_step_counter)\n",
        "metrics = sim.train()\n",
        "sim.plot_performance(metrics)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pTeb9dL6H0Cu"
      },
      "source": [
        "# Conclusion\n",
        "\n",
        "CONCLUDE"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bBrMDUgZH0Cv"
      },
      "source": [
        "## References\n",
        "\n",
        "- [TensorFlow Agents](https://www.tensorflow.org/agents/overview)\n",
        "- [Open Gym AI Github](https://github.com/openai/gym)\n",
        "- [Greg et al, OpenAI Gym, (2016)](https://arxiv.org/abs/1606.01540)\n",
        "- [Théate, Thibaut, and Damien Ernst. \"An application of deep reinforcement learning to algorithmic trading.\" Expert Systems with Applications 173 (2021): 114632.](https://www.sciencedirect.com/science/article/pii/S0957417421000737)\n",
        "- [Remote development in WSL](https://code.visualstudio.com/docs/remote/wsl-tutorial)\n",
        "- [NVIDIA Driver Downloads](https://www.nvidia.com/Download/index.aspx)\n",
        "- [CUDA Toolkit](https://developer.nvidia.com/cuda-toolkit-archive)\n",
        "- [TensorRT for CUDA](https://docs.nvidia.com/deeplearning/tensorrt/archives/index.html#trt_7)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kIgjl92lH0Cv"
      },
      "source": [
        "## Github\n",
        "\n",
        "Article here is also available on [Github](https://github.com/adamd1985/pairs_trading_unsupervised_learning)\n",
        "\n",
        "Kaggle notebook available [here](https://www.kaggle.com/code/addarm/unsupervised-learning-as-signals-for-pairs-trading)\n",
        "\n",
        "## Media\n",
        "\n",
        "All media used (in the form of code or images) are either solely owned by me, acquired through licensing, or part of the Public Domain and granted use through Creative Commons License.\n",
        "\n",
        "## CC Licensing and Use\n",
        "\n",
        "<a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\"><img alt=\"Creative Commons License\" style=\"border-width:0\" src=\"https://i.creativecommons.org/l/by-nc/4.0/88x31.png\" /></a><br />This work is licensed under a <a rel=\"license\" href=\"http://creativecommons.org/licenses/by-nc/4.0/\">Creative Commons Attribution-NonCommercial 4.0 International License</a>."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.8"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}